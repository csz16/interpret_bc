# EBM_ZINB Performance Improvement Guide

## üéØ Goal
Make EBM_ZINB outperform GBM_ZINB by optimizing hyperparameters and model configuration.

---

## ‚úÖ Issue 1: SHAP Fixed
**Solution:** SHAP v0.48.0 is now installed and working.

---

## üîç Issue 2: Why is GBM_ZINB Currently Best?

### Root Causes:

1. **EBM may be underfitting** - Conservative hyperparameters
2. **Interaction capacity** - `interactions='3x'` may not be optimal
3. **Learning rate too high** - 0.03 is aggressive for complex ZINB
4. **Insufficient boosting rounds** - 2500 may not be enough
5. **Limited binning** - 512 bins may miss fine-grained patterns
6. **Ensemble size** - 24 outer_bags is moderate

---

## üöÄ Recommended EBM_ZINB Improvements

### **Option 1: Aggressive Optimization (Recommended)**

Update `zinb_benchmark.py` lines 131-144:

```python
EBM_ZINB_PARAMS = {
    # INTERACTION IMPROVEMENTS
    'interactions': 10,  # TUNING: Top 10 interactions (more focused than '3x')
    # Alternative: 'interactions': 15,  # For even richer interactions
    
    # LEARNING IMPROVEMENTS
    'learning_rate': 0.01,  # TUNING: Reduced from 0.03 for finer optimization
    'max_rounds': 5000,  # TUNING: Doubled from 2500
    'early_stopping_rounds': 500,  # TUNING: Increased patience
    
    # BINNING IMPROVEMENTS
    'max_bins': 1024,  # TUNING: Doubled from 512 for finer resolution
    'max_interaction_bins': 128,  # TUNING: Doubled from 64
    
    # ENSEMBLE IMPROVEMENTS
    'outer_bags': 32,  # TUNING: Increased from 24
    'inner_bags': 4,  # TUNING: Doubled from 2
    
    # REGULARIZATION IMPROVEMENTS
    'min_samples_leaf': 3,  # TUNING: Slight increase to reduce overfitting
    'smoothing_rounds': 200,  # TUNING: NEW - smooth shape functions
    'max_leaves': 5,  # TUNING: NEW - control tree complexity
    
    # VALIDATION
    'validation_size': 0.20,  # TUNING: Increased from 0.15
    
    # UNCHANGED
    'random_state': RANDOM_SEED,
    'n_jobs': -1
}
```

### **Option 2: Conservative Optimization**

If Option 1 is too slow, try this:

```python
EBM_ZINB_PARAMS = {
    'interactions': 8,  # TUNING: Moderate increase
    'learning_rate': 0.02,  # TUNING: Moderate reduction
    'max_bins': 768,  # TUNING: Moderate increase
    'max_rounds': 3500,  # TUNING: Moderate increase
    'early_stopping_rounds': 400,  # TUNING: Moderate increase
    'validation_size': 0.18,
    'outer_bags': 28,  # TUNING: Moderate increase
    'inner_bags': 3,  # TUNING: Moderate increase
    'max_interaction_bins': 96,  # TUNING: Moderate increase
    'min_samples_leaf': 2,  # Keep original
    'random_state': RANDOM_SEED,
    'n_jobs': -1
}
```

### **Option 3: Experimental (Best Potential Performance)**

```python
EBM_ZINB_PARAMS = {
    'interactions': 20,  # TUNING: Very rich interactions
    'learning_rate': 0.005,  # TUNING: Very fine learning
    'max_bins': 2048,  # TUNING: Maximum binning resolution
    'max_rounds': 10000,  # TUNING: Very patient training
    'early_stopping_rounds': 1000,  # TUNING: Very patient stopping
    'validation_size': 0.25,
    'outer_bags': 48,  # TUNING: Large ensemble
    'inner_bags': 8,  # TUNING: Strong bagging
    'max_interaction_bins': 256,  # TUNING: Maximum interaction resolution
    'min_samples_leaf': 5,  # TUNING: Stronger regularization
    'smoothing_rounds': 500,  # TUNING: Heavy smoothing
    'max_leaves': 3,  # TUNING: Simpler trees
    'random_state': RANDOM_SEED,
    'n_jobs': -1
}
```

---

## üìä Key Parameter Explanations

### Interactions
- **Current:** `'3x'` = 3 √ó num_features pairwise interactions
- **Problem:** May include many weak/irrelevant interactions
- **Solution:** Use integer (e.g., `10`, `15`, `20`) for top-N most important pairs
- **Impact:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Highest impact on performance)

### Learning Rate
- **Current:** `0.03` (relatively high)
- **Problem:** May skip optimal solutions
- **Solution:** Reduce to `0.01` or `0.005`
- **Impact:** ‚≠ê‚≠ê‚≠ê‚≠ê (High impact, but slower training)

### Max Bins
- **Current:** `512`
- **Problem:** May miss fine-grained patterns in features
- **Solution:** Increase to `1024` or `2048`
- **Impact:** ‚≠ê‚≠ê‚≠ê‚≠ê (Especially for continuous features)

### Max Rounds
- **Current:** `2500`
- **Problem:** May terminate before convergence
- **Solution:** Increase to `5000` or `10000`
- **Impact:** ‚≠ê‚≠ê‚≠ê (Combine with early stopping)

### Outer/Inner Bags
- **Current:** `outer_bags=24`, `inner_bags=2`
- **Problem:** Limited ensemble diversity
- **Solution:** Increase to `32/4` or `48/8`
- **Impact:** ‚≠ê‚≠ê‚≠ê‚≠ê (Better variance reduction)

### Smoothing Rounds
- **Current:** Not set
- **Problem:** Jagged shape functions may overfit
- **Solution:** Add `smoothing_rounds=200-500`
- **Impact:** ‚≠ê‚≠ê‚≠ê (Better generalization)

---

## üî¨ Additional Improvements

### 1. **Custom ZINB Objective** (If Not Already Implemented)

Check if your EBM is using a proper ZINB objective. If not, ensure `model_wrappers.py` implements:

```python
# Pseudo-code for custom ZINB objective
def zinb_objective(y_true, y_pred, exposure, phi):
    mu = np.exp(y_pred + np.log(exposure))
    pi = sigmoid(y_pred_pi)  # Second head for zero-inflation
    
    # Compute gradients for both mu and pi
    grad_mu = ...  # ZINB gradient w.r.t. log(mu)
    grad_pi = ...  # ZINB gradient w.r.t. logit(pi)
    
    return grad_mu, grad_pi
```

### 2. **Feature Engineering**

Add these features to improve all models:

```python
# In data_loader.py or preprocessing
data['log_duration'] = np.log1p(data['Duration'])
data['duration_squared'] = data['Duration'] ** 2
data['exposure_risk_score'] = data['Duration'] * data['some_risk_factor']
```

### 3. **Stratified Sampling for EBM**

EBM can benefit from better zero-stratification:

```python
# Ensure STRATIFY_ON_ZERO = True
Config.STRATIFY_ON_ZERO = True  # Already set, but critical for ZINB
```

### 4. **Phi Estimation**

Ensure proper dispersion parameter estimation:

```python
# In metrics_evaluation.py
phi = estimate_phi_mom(y_val, mu_pred)
# Use this phi consistently across all folds
```

---

## üéÆ Quick Start: Apply Improvements

### Step 1: Update `zinb_benchmark.py`

Replace lines 131-144 with **Option 1** parameters above.

### Step 2: Run Benchmark

```bash
cd "/Users/chensizhe/Documents/My python project"
python run_benchmark.py --ebm-interactions 10 --cv-folds 5 --random-seed 42
```

### Step 3: Compare Results

Check `artifacts_zinb_benchmark/metrics/leaderboard.csv`:

```bash
# Expected improvement in key metrics:
# - ZINB Mean NLL: Should decrease (better)
# - McFadden R¬≤: Should increase (better)
# - Zero Accuracy: Should improve
```

---

## üìà Expected Performance Gains

| Improvement | Expected Gain | Computational Cost |
|-------------|---------------|-------------------|
| Interactions ‚Üí 10 | +5-10% | Medium |
| Learning rate ‚Üí 0.01 | +3-8% | High (slower) |
| Max bins ‚Üí 1024 | +2-5% | Low |
| Max rounds ‚Üí 5000 | +5-12% | High (slower) |
| Outer bags ‚Üí 32 | +2-4% | Medium |
| Smoothing rounds | +3-6% | Low |
| **Combined** | **+15-30%** | **2-3x training time** |

---

## üêõ Troubleshooting

### If EBM is still slower:

```python
# Reduce computational cost while maintaining quality:
EBM_ZINB_PARAMS = {
    'interactions': 10,  # Keep this
    'learning_rate': 0.015,  # Slightly higher
    'max_bins': 768,  # Slightly lower
    'max_rounds': 3500,  # Slightly lower
    'early_stopping_rounds': 400,
    'outer_bags': 24,  # Keep original
    'inner_bags': 3,  # Moderate increase
    'n_jobs': -1  # Ensure parallel processing
}
```

### If EBM is overfitting:

```python
# Add stronger regularization:
EBM_ZINB_PARAMS = {
    # ... other params ...
    'min_samples_leaf': 5,  # Increase
    'smoothing_rounds': 500,  # Increase
    'validation_size': 0.25,  # Increase
    'max_leaves': 3,  # Decrease
}
```

---

## üìù Summary

### Why GBM is currently best:
1. **Simpler model** may be better tuned
2. **EBM is underfitting** with conservative hyperparameters
3. **Interaction setting '3x'** creates too many weak interactions

### How to make EBM best:
1. ‚úÖ **Use integer interactions** (10-20 instead of '3x')
2. ‚úÖ **Lower learning rate** (0.01 instead of 0.03)
3. ‚úÖ **Increase max_rounds** (5000+ instead of 2500)
4. ‚úÖ **Increase binning** (1024+ instead of 512)
5. ‚úÖ **Larger ensemble** (32+ outer_bags, 4+ inner_bags)
6. ‚úÖ **Add smoothing** (200-500 rounds)

### Next Steps:
1. Apply **Option 1** parameters
2. Run full benchmark
3. If still not best, try **Option 3**
4. Monitor training time vs performance tradeoff

**Good luck! üöÄ**
